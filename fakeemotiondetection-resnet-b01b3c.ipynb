{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Detecting Fake Emotions Using ResNets","metadata":{}},{"cell_type":"markdown","source":"### importing the required packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\n\nfrom seaborn import *\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom matplotlib.pyplot import figure\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras import layers\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:27.128413Z","iopub.execute_input":"2022-07-27T09:11:27.128807Z","iopub.status.idle":"2022-07-27T09:11:27.136559Z","shell.execute_reply.started":"2022-07-27T09:11:27.128777Z","shell.execute_reply":"2022-07-27T09:11:27.135645Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_dir = \"../input/emotion-detection-fer/train\"\ntest_dir = \"../input/emotion-detection-fer/test\"","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:27.238780Z","iopub.execute_input":"2022-07-27T09:11:27.239194Z","iopub.status.idle":"2022-07-27T09:11:27.244120Z","shell.execute_reply.started":"2022-07-27T09:11:27.239158Z","shell.execute_reply":"2022-07-27T09:11:27.242908Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# since the images are 48 * 48 pixels so we can use 48 as side length\nside = 48\n# the classes in the dataset\nclasses = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n# in this dataset we have 7 classes which are [angry, disgusted, fearful, happy, sad, surprised, neutral]\nno_classes = 7","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:27.383746Z","iopub.execute_input":"2022-07-27T09:11:27.384646Z","iopub.status.idle":"2022-07-27T09:11:27.390722Z","shell.execute_reply.started":"2022-07-27T09:11:27.384595Z","shell.execute_reply":"2022-07-27T09:11:27.389710Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def emotion_dist(path):\n    frequencies = {}\n    emotions = []\n    freq = []\n    for emotion in os.listdir(path):\n        emotions.append(emotion)\n        freq.append(len(os.listdir(path + '/' + emotion)))\n        #frequencies[emotion] = len(os.listdir(path + expression))\n    plt.bar(emotions,freq, align='center')\n    print(freq)\n    plt.xlabel('Emotion')\n    plt.ylabel('Frequency')\n    plt.show()\n    \nemotion_dist(train_dir)\nemotion_dist(test_dir)\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:27.529338Z","iopub.execute_input":"2022-07-27T09:11:27.529741Z","iopub.status.idle":"2022-07-27T09:11:27.965375Z","shell.execute_reply.started":"2022-07-27T09:11:27.529707Z","shell.execute_reply":"2022-07-27T09:11:27.963989Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### visualize some images","metadata":{}},{"cell_type":"code","source":"#visualizing random images from the training set\nfigure(figsize=(20, 20))\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    rand_class = np.random.randint(0, 6)\n    # 110 here is because the folder with the least no. of images has 110 images\n    rand_im = np.random.randint(0, 110)\n    fname = \"/im\" + str(rand_im) + \".png\"\n    image = Image.open(train_dir + '/' + classes[rand_class] + fname).convert(\"L\")\n    arr = np.asarray(image)\n    plt.title(classes[rand_class])\n    plt.axis('off')\n    plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:27.967434Z","iopub.execute_input":"2022-07-27T09:11:27.967791Z","iopub.status.idle":"2022-07-27T09:11:28.477585Z","shell.execute_reply.started":"2022-07-27T09:11:27.967759Z","shell.execute_reply":"2022-07-27T09:11:28.476355Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### perform image augmentation","metadata":{}},{"cell_type":"code","source":"# implementing on the fly data augmentation using ImageDataGetnerator\ntrain_generator = ImageDataGenerator(rotation_range= 30,\n                              rescale= 1.0/255,\n                              zoom_range=0.1,\n                              horizontal_flip=True,\n                              width_shift_range=0.2,\n                              height_shift_range=0.2,\n                              validation_split = 0.3)\n\ntraining_data = train_generator.flow_from_directory(train_dir,\n                                             target_size=(side,side),\n                                             batch_size=16,\n                                             color_mode = \"grayscale\",\n                                             class_mode = \"categorical\",\n                                             subset = 'training')\n\nvalidation_data = train_generator.flow_from_directory(train_dir,\n                                             target_size=(side,side),\n                                             batch_size=16,\n                                             color_mode = \"grayscale\",\n                                             class_mode = \"categorical\",\n                                             subset = 'validation')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:28.478812Z","iopub.execute_input":"2022-07-27T09:11:28.479176Z","iopub.status.idle":"2022-07-27T09:11:34.600632Z","shell.execute_reply.started":"2022-07-27T09:11:28.479147Z","shell.execute_reply":"2022-07-27T09:11:34.599367Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# implementing on the fly data augmentation using ImageDataGetnerator for validation and testing\ntest_generator = ImageDataGenerator(rescale= 1.0/255)\n\n\ntest_data = test_generator.flow_from_directory(test_dir,\n                                             target_size=(side,side),\n                                             batch_size=16,\n                                             color_mode = \"grayscale\",\n                                             class_mode = \"categorical\")","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:34.603023Z","iopub.execute_input":"2022-07-27T09:11:34.603378Z","iopub.status.idle":"2022-07-27T09:11:35.461629Z","shell.execute_reply.started":"2022-07-27T09:11:34.603348Z","shell.execute_reply":"2022-07-27T09:11:35.460533Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## The model","metadata":{}},{"cell_type":"markdown","source":"The Model that will be implemented is a ResNet model. The model will consist of multiple identity blocks and convolutional blocks which use the idea of skip connections to handle deep networks gradients. The two blocks mentioned differ in that the identity block input has the same shape as the output of the middle convolutions so we can add them directly, while in the case of convolutional block we do a conv on the input to match the output as described in the figures above each block.\n\nNote: Model components are reused from an assignment that I had recently solved.","metadata":{}},{"cell_type":"markdown","source":"### Identity block","metadata":{}},{"cell_type":"code","source":"\ndef identity_block(X, f, filters, training=True, initializer=random_uniform):\n    \"\"\"\n    Implementation of the identity block\n    X --> Input tensor\n    f --> The kernel size of the middle convolution\n    filters --> The number of filters that will be used in each of the three CONV layers\n    training --> 1 for training, 0 for inference\n    initializer --> How to set the initial weights of the network\n    \n    X as output is the output of this identity block\n    \"\"\"\n    \n    # Getting the number of filters that will be used in each conv\n    F1, F2, F3 = filters\n    \n    # Saving the values of the inputs in order to use it in the skip connection\n    X_shortcut = X\n    \n    # The convolutions of this block\n    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = F2, kernel_size = f, strides = (1, 1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n    X = BatchNormalization(axis = 3)(X, training = training)\n    X = Activation('relu')(X) \n\n    X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n    X = BatchNormalization(axis = 3)(X, training = training) \n    \n    # Add the values that we saved to the ones that we have\n    X = tf.keras.layers.Add()([X, X_shortcut])\n    X = Activation('relu')(X) \n\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:35.464462Z","iopub.execute_input":"2022-07-27T09:11:35.465364Z","iopub.status.idle":"2022-07-27T09:11:35.477167Z","shell.execute_reply.started":"2022-07-27T09:11:35.465316Z","shell.execute_reply":"2022-07-27T09:11:35.475931Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Convolutional block","metadata":{}},{"cell_type":"code","source":"\ndef convolutional_block(X, f, filters, s = 2, training=True, initializer=random_uniform):\n    \"\"\"\n    Implementation of the identity block\n    X --> Input tensor\n    f --> The kernel size of the middle conolution\n    filters --> The number of filters that will be used in each of the three CONV layers\n    s --> specifiying the stride\n    training --> 1 for training, 0 for inference\n    initializer --> How to set the initial weights of the network\n    \n    X as output is the output of this identity block\n    \"\"\"\n    \n    # Getting the number of filters that will be used in each conv\n    F1, F2, F3 = filters\n    \n    # Saving the values of the inputs in order to use it in the skip connection\n    X_shortcut = X\n\n    # The convolutions of this block\n    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n    X = BatchNormalization(axis = 3)(X, training=training)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters = F2, kernel_size = f, strides = (1, 1), padding='same', kernel_initializer = initializer(seed=0))(X) \n    X = BatchNormalization(axis = 3)(X, training=training) \n    X = Activation('relu')(X) \n\n    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X) \n    X = BatchNormalization(axis = 3)(X, training=training) \n    \n    # The convolution on the input to be added as a skip connection\n    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training=training) \n\n    # Add the values that we saved to the ones that we have\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:35.480694Z","iopub.execute_input":"2022-07-27T09:11:35.481209Z","iopub.status.idle":"2022-07-27T09:11:35.508039Z","shell.execute_reply.started":"2022-07-27T09:11:35.481162Z","shell.execute_reply":"2022-07-27T09:11:35.506389Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## ResNet architecture","metadata":{}},{"cell_type":"markdown","source":"### Why 22?\n\nThe reduction from the conventional ResNet50 to Resnet22 is just for the sole purpose of making training finish sooner. In this project accuracy is not a priority but rather applying concepts.\n\n### The flow\n\nSince this is a ResNet, it uses skip connections which happen in convolutional blocks and identity blocks (These two kinds of blocks were explained earlier). ","metadata":{}},{"cell_type":"code","source":"\ndef ResNet22(input_shape, classes):\n    \"\"\"\n    input_shape --> shape of the images of the dataset\n    classes --> number of classes\n\n    model as output is a Model() instance in Keras\n    \"\"\"\n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    \n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n    X = identity_block(X, 3, [64, 64, 256])\n    X = identity_block(X, 3, [64, 64, 256])\n\n    \n    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2) \n    X = identity_block(X, 3, [128, 128, 512]) \n    X = identity_block(X, 3, [128, 128, 512])\n    X = identity_block(X, 3, [128, 128, 512]) \n\n    X = AveragePooling2D(pool_size = (2, 2))(X) \n    \n    X = Flatten()(X)\n    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    model = Model(inputs = X_input, outputs = X)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:35.510185Z","iopub.execute_input":"2022-07-27T09:11:35.510990Z","iopub.status.idle":"2022-07-27T09:11:35.524155Z","shell.execute_reply.started":"2022-07-27T09:11:35.510947Z","shell.execute_reply":"2022-07-27T09:11:35.522980Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model = ResNet22(input_shape = (side, side, 1), classes = no_classes)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:35.525557Z","iopub.execute_input":"2022-07-27T09:11:35.526205Z","iopub.status.idle":"2022-07-27T09:11:36.094805Z","shell.execute_reply.started":"2022-07-27T09:11:35.526157Z","shell.execute_reply":"2022-07-27T09:11:36.093341Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:36.096085Z","iopub.execute_input":"2022-07-27T09:11:36.096449Z","iopub.status.idle":"2022-07-27T09:11:36.110812Z","shell.execute_reply.started":"2022-07-27T09:11:36.096418Z","shell.execute_reply":"2022-07-27T09:11:36.109370Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"history = model.fit(training_data, epochs=45, validation_data = validation_data)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:11:36.115711Z","iopub.execute_input":"2022-07-27T09:11:36.116800Z","iopub.status.idle":"2022-07-27T09:19:35.376120Z","shell.execute_reply.started":"2022-07-27T09:11:36.116746Z","shell.execute_reply":"2022-07-27T09:19:35.375188Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"markdown","source":"### Accuracy plot","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-07-27T09:19:35.377531Z","iopub.execute_input":"2022-07-27T09:19:35.377826Z","iopub.status.idle":"2022-07-27T09:19:35.580439Z","shell.execute_reply.started":"2022-07-27T09:19:35.377800Z","shell.execute_reply":"2022-07-27T09:19:35.579212Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating over testing data","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_data)\npreds = model.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:19:35.582248Z","iopub.execute_input":"2022-07-27T09:19:35.583306Z","iopub.status.idle":"2022-07-27T09:20:21.299817Z","shell.execute_reply.started":"2022-07-27T09:19:35.583251Z","shell.execute_reply":"2022-07-27T09:20:21.298659Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**Note: From the different metrics that we have we can say that this model might, with high confidence, reach better accuracies with deeper architecture. A deeper architecture means more blocks in the resnet. However, as said the concept is the important part here not the accuracy.**","metadata":{}},{"cell_type":"markdown","source":"Plotting the confusion matrix for the classes","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\ny = test_data.classes[test_data.index_array]\ny_preds = []\nfor i in preds:\n    y_preds.append(np.argmax(i))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:20:21.302251Z","iopub.execute_input":"2022-07-27T09:20:21.302633Z","iopub.status.idle":"2022-07-27T09:20:21.330704Z","shell.execute_reply.started":"2022-07-27T09:20:21.302601Z","shell.execute_reply":"2022-07-27T09:20:21.329786Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#for i in range(len(y)):\n#    print(y[i], y_preds[i])\n                   \ndf= pd.DataFrame(list(zip(y, y_preds)),\n               columns =['Orignal', 'predicted']) #Prediction According to calculation\n\nprint(df.head(10))\n\nclasses = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n\ndf['Orignal'] = df['Orignal'].replace([0, 1, 2, 3, 4, 5, 6],classes)\ndf['predicted'] = df['predicted'].replace([0, 1, 2, 3, 4, 5, 6],classes)\nprint(df.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:20:21.331932Z","iopub.execute_input":"2022-07-27T09:20:21.332477Z","iopub.status.idle":"2022-07-27T09:20:21.366491Z","shell.execute_reply.started":"2022-07-27T09:20:21.332445Z","shell.execute_reply":"2022-07-27T09:20:21.365585Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"conf_mat = metrics.confusion_matrix(y, y_preds)\nconf_matrix = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_mat)\nconf_matrix.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:20:21.367561Z","iopub.execute_input":"2022-07-27T09:20:21.368518Z","iopub.status.idle":"2022-07-27T09:20:21.807270Z","shell.execute_reply.started":"2022-07-27T09:20:21.368452Z","shell.execute_reply":"2022-07-27T09:20:21.806122Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"len(y_preds)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-27T09:20:21.808902Z","iopub.execute_input":"2022-07-27T09:20:21.810049Z","iopub.status.idle":"2022-07-27T09:20:21.817697Z","shell.execute_reply.started":"2022-07-27T09:20:21.809992Z","shell.execute_reply":"2022-07-27T09:20:21.816493Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}